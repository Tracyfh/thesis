\chapter{Conclusions, Discussion and Recommendations}
\label{ch8-conclusion}

\dropcap{T}{his} concluding chapter explains the scientific and technical implications for the academic community and society of the research findings. 

Automated surface water detection from satellite imagery remains a challenging task, especially in situations, where only a limited number of satellite images is available. In this cases, the use of multiple data sources, combining imagery from multiple satellite sensors, is feasible. Eventhough this approach may require the use of algorithms that are tuned for every satellite sensor. I'm convinced, that the first step in every study involving multitemporal satellite data should be to generate and visually inspect time lapse videos based on all available satellite images for a given study area. This way, variability of reflectance values can be visually inspected in order to develop an optimal strategy for further algorithm selection and tuning. Mostly, these videos will be polluted with atmospheric noise and all kinds of artifacts, caused by sensor limitations or data processing inconsistencies. For many surface water studies, atmospheric effects can be easily eliminated by producing simple percentile composite images instead of the real ones, as discussed in chapters \ref{ch5} and \ref{ch7}. Parameterizing algorithms to use typical cloud frequency for a given area to tune this kind of averaging sounds a feasible technique. This way, long-term trends can be easily explored, to help better understand how the algorithms, such as discussed throughout this thesis, can be further tuned to focus on specific natural or human-made processes. Understanding longer-term trends is useful for a more specific tuning of more expensive algorithms, focusing on processing of satellite images on a per-scene basis. Such algorithms are discussed in chapters \ref{ch3} and \ref{ch4}. However, scaling this kind of algorithms to perform processing globally, at high spatial and temporal resolution, using both passive multispectral and active \gls{SAR} sensors, can be a tremendous task, facing many technical and methodological limitations.

In this thesis, a number of algorithms to overcome these limitations were discussed, either by reviewing existing methods, as discussed in Chapter \ref{ch2}, or by introducing new algorithms, useful for many surface water detection applications. Some of these algorithms focus on the elimination of effects caused by clouds, cloud shadows, snow/ice or hills, while others provide a way for a more accurate or more resource-efficient detection of surface water from noisy satellite imagery.

A new discriminative method was developed using two popular image processing algorithms: Canny Edge filter and Otsu thresholding. The method allows very accurate surface water detection. The method was further extended with a generative step, allowing  to infer surface water for non-observed parts of the images, either due to limited swath or due to presence of noisy pixels, covered by clouds, snow, ice, very dark shadows, and vegetation. The method has been applied to reconstruct surface water dynamics of a reservoir, where all freely available multi-sensor imagery was used with a spatial resolution higher or equal to 30m. The validation shows excellent performance of the method. 

We have discussed the potential of the use of Bayesian methods to infer a set of unobserved or indirectly observed variables from multi-temporal satellite data. This kind of technique is needed when applying the method at global scale. The Bayesian framework introduced in the thesis can help detecting surface water for partially-observed water bodies. The resulting reconstructed water masks perfectly match with the observed water level data. Even more, the satellite-derived water surface water area values allow detection of sensor artifacts, where measurements were not available. 

For an automated long-term surface water change detection, we have developed a simple method, based on trend analysis of wetness indices computed for reflectance percentile composites. A very simple yet robust algorithm allows very accurate detection of surface water changes at global scale, provided that sufficient observations are available to ensure continuity in statistical properties of the analyzed image collections. The method was then applied to perform a global surface water change study and to develop a software tool called Aqua Monitor.

To further investigate the applicability of the methods and to compare satellite-derived surface water masks to existing global surface water datasets, surface water of the Murray-Darling Basin in Australia was studied. Here, the local dynamic thresholding method was applied to process Landsat 8 reflectance percentile composite images to detect permanent surface water mask at 30m resolution. The resulting water mask, together with the newly-generated 30m-resolution drainage network derived from \gls{SRTM}, was compared to the water mask extracted from OSM.

The results reveal that the best surface water mask should be generated by combining all three data sets that were analyzed in the current study; water masks extracted from OSM, optical satellite imagery and the drainage network derived from high-resolution digital elevation models for hilly areas.

A good agreement was found, concerning positional accuracy, between river water features from OSM and water masks derived from Landsat 8. However, only 32\% of the total OSM and Landsat 8 water mask matches when analyzing the actual intersecting surface area.

The newly generated Landsat 8 water mask reveals many new water bodies previously not present in OSM or any other vector dataset we have explored. A large part constitutes a large number of small agricultural reservoirs located in the northern and southern parts of the catchment.

\section{Outlook}

The use of all available satellite imagery from multiple passive optical and active radar sensors increases our chances to detect the actual variability of surface water. However, in many cases the amount of information observed from satellites may be insufficient to represent all dynamics. Passive multispectral images may be useless in case of thick clouds, when no land surface can be observed. Therefore, combining information extracted from \gls{EO} data with the actual local data, including alternative sources such as social media, may be feasible to reconstruct the actual surface water dynamics. This becomes even more relevant for real-time applications, for example, when information on flood extent is needed to better deal with emergency situations.

Furthermore, the number of free \gls{EO} observations available increases exponentially, challenging remote sensing researchers to develop more effective methods to extract higher level variables from these datasets in an automated way. Effective infrastructures allowing to store, but also to analyze these data will be crucial to process these datasets.

While doing this research, I've been faced with the challenges that the amount of the new datasets and new data science methods to address remote sensing or more general classification questions develop very rapidly. Therefore, a lot of focus was shifted towards development of new methods and exploratory tools instead of derivation of new higher-level datasets. With the introduction of tools such as Google Earth Engine, processing of huge amounts of \gls{EO} data have become a reality. 

\begin{comment}

\subsection{Estimation of water level changes from satellite altimetry}
\url{https://www.pecad.fas.usda.gov/cropexplorer/global_reservoir/}
\url{http://dahiti.dgfi.tum.de/en/}
\url{http://www.legos.obs-mip.fr/en/soa/hydrologie/hydroweb/}

... cross-validation, recnostruction of bathymetry (accuracy?)

\subsection{Segmentation of multi-spectral images using unsupervised methods}
Discuss the main issues related to unsupervised segmentation and how it can be used to improve multi-spectral image classification (dimensionality reduction, speed, bandwidth).

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{01.5-unsupervised/figures/superpixels15m}
	\caption{Original 30m resolution image (left), resampled with bicubic method to 15m and segmented (middle), and clustered, showing average values and centroids for superpixels (right)}
\end{figure}

\subsection{Surface water detection using a mix of graph-cuts and Bayesian methods}
Define energy function to detect water.

\begin{figure}[H]
	\centering
	\includegraphics[height=3in]{01.5-unsupervised/figures/slic-graph-cuts}
	\caption{Example of application of a simple graph-cut method to discriminate surface water from segmented image}
\end{figure}

\end{comment}

\section{Recommendations}

Further development of multi-sensor Bayesian methods is feasible, taking into account changes in the waterbody configuration, either due to natural processes or human activities (meandering rivers, sedimentation of reservoirs, land reclamation and so on). This kind of changes would be possible to incorporate into the Bayesian framework introduced in the Chapter \ref{ch4} by making density functions (prior) time-dependent. Adding temporal variability may be useful to further increase accuracy and transferability of the method even when no such changes occur to the waterbody, but due to seasonal changes. Further extending of these methods to include temporal correlation between measurements may be needed to eliminate the effect of outliers. This can, for example, be done by incorporating sequence learning methods, such as for example Long short-term memory (LSTM) \citet{hochreiter1997long}.

One important aspect to be taken into account when detecting surface water form \gls{EO} data at the planetary scale is that all of the steps need to be automated. This can be achieved by the use of fully automated methods. Prospective directions include the use of unsupervised machine learning methods, such as efficient image segmentation methods like SLIC \citet{achanta2012slic} combined with the use of automated inference methods to perform further classification of these clustered images. The use of more suitable computational hardware and software platforms to speed-up data processing \citet{donchyts2017slic} is important to allow researchers to explore faster and measure performance of these approaches.

The parallel processing platform Google Earth Engine has opened a new era of \gls{EO} data processing by allowing researchers around the world to freely process petabytes of satellite data. Built on simple algorithms as components, which are easy to reproduce locally, the platform allows development of new \gls{EO} algorithms that can be scaled easily to perform global analysis at high spatio-temporal resolution. The platform solves many technical problems , but also demands new way of working with the data and algorithms.

Surface water detection in hilly areas can be challenging, especially when no recent DEM is available or when its vertical accuracy is low. In this case, the use of topographic indices such as \gls{HAND} \citet{Nobre2011} may help to eliminate false-positive surface water pixels or to identify areas where additional processing may be required. For single-image processing in topographically complex areas the use of topographic correction methods may be feasible, as discussed in the Chapter \ref{ch2}. However, applicability of these methods to study long-term changes at large spatial extents may be too resource-demanding.

For better applicability of statistical models, the incorporation of cloud frequency as one of the variables used for classification methods can help in implementing automated methods of surface water detection. Knowing how cloud frequency varies will allow to better adjust methods such as introduced in the Chapter \ref{ch5}. Temporal variability of the cloud frequency can help to better parameterize the statistical algorithms to generate the range of reflectance percentile composites to study variability of the land surface processes.

In addition to auxiliary variables such as cloud frequency, the use of aggregated, upscaled representations of \gls{EO} datasets may be useful to help performing inference faster. In classical geospatial applications, the use of coarser representations of raster data (pyramids) is a common technique to reduce time required for data processing at coarser scale. However, the same approach can be used to generate aggregated representations for \gls{EO} datasets. This should help to perform image classification much faster when compared to existing methods. Eventhough this is more a technical question, it can be frequently crucial to perform some data operations very fast, allowing researchers to focus on more fundamental questions.

In the present thesis I have investigated applicability of the new methods to freely available \gls{EO} data. But nowadays, many commercial datasets, with much higher resolution (spatial, temporal or spectral) can be acquired. Therefore, one of the future directions can be to investigate the optimal type of dataset required for every time of the study. In many cases, the use of free \gls{EO} data may be sufficient to answer many questions. But for other studies, the resolution may be insufficient to represent variability of the observed natural or human-made processes. In some cases, a hybrid approach may be feasible, where a limited amount of high-resolution data can be used to improve predictive quality of the algorithms. Finding an optimal strategy depends strongly on the goal of the study.

For some natural processes, mixing of EO-derived variables with the best physically-based numerical models can be useful to restore the sequence of surface water changes. One of the challenges in this case will be a compromise between computational and storage resources required to use numerical models. In some cases, the use of modern machine learning techniques may help to get better insights.
